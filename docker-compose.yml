version: '3.9'

services:
  rag_api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: rag_api
    ports:
      - "8000:8000"
    env_file:
      - .env
    command: >
      uvicorn api.main:app --host 0.0.0.0 --port 8000
    volumes:
      - .:/app
      - ~/models/Meta-Llama-3.1-8B-Instruct:/models/Meta-Llama-3.1-8B-Instruct
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]

  rag_ui:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: rag_ui
    ports:
      - "8501:8501"
    env_file:
      - .env
    command: >
      streamlit run streamlit_chat.py --server.port=8501 --server.address=0.0.0.0
    volumes:
      - .:/app
      - ~/models/Meta-Llama-3.1-8B-Instruct:/models/Meta-Llama-3.1-8B-Instruct
    restart: unless-stopped
