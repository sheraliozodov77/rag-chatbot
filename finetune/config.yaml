### Model
model_name: llama3_8b_instruct
model_name_or_path: meta-llama/Meta-Llama-3.1-8B-Instruct

### Method
stage: sft
do_train: true
finetuning_type: lora
lora_target: all
lora_rank: 64
lora_alpha: 16
lora_dropout: 0.1
create_new_adapter: true

### Dataset
dataset: uzbek_data
template: llama3
cutoff_len: 2048
max_samples: 10000
overwrite_cache: true
preprocessing_num_workers: 8

### Quantization
quantization_bit: 4
quantization_type: nf4
double_quantization: true
compute_dtype: float16

### Training
per_device_train_batch_size: 2
gradient_accumulation_steps: 8
learning_rate: 2.0e-4
num_train_epochs: 3
lr_scheduler_type: cosine
warmup_steps: 100
fp16: true
gradient_checkpointing: true

### Optimization
optim: adamw_torch
weight_decay: 0.01
max_grad_norm: 1.0
group_by_length: true
dataloader_num_workers: 4

### Logging
output_dir: ./saves/llama3_8b_uzbek_qlora
logging_steps: 10
save_steps: 500
save_total_limit: 3
report_to: [wandb, mlflow] 
run_name: llama3_8b_uzbek_qlora

mlflow_tracking_uri: ./mlruns
mlflow_run_name: llama3_uzbek_run1